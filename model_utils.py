# -*- coding: utf-8 -*-
"""model_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pG3fP0ZkEJcmXf97m5-w6aK2BhKB5Awf
"""

import pandas as pd
import logging

def align_features(input_df, model):
    for f in model.feature_names_in_:
        if f not in input_df:
            input_df[f] = 0
    return input_df[model.feature_names_in_]

def compute_mean_for_teams(home, away, data, model, get_column_names, version="v1"):
    home_col, away_col, result_col = get_column_names(version)
    h2h = data[(data[home_col] == home) & (data[away_col] == away)]
    if h2h.empty:
        return None
    h2h = h2h.drop(columns=[result_col, "Date", "Country", "League", "Season", "Time"], errors='ignore')
    if version == "v1" and 'HTR' in h2h:
        h2h['HTR'] = h2h['HTR'].replace({'H': 1, 'D': 2, 'A': 3})
    mean = h2h.mean(numeric_only=True)
    if 'HTR' in mean:
        if 0 <= mean['HTR'] <= 1.4:
            mean['HTR'] = 'H'
        elif 1.5 <= mean['HTR'] <= 2.4:
            mean['HTR'] = 'D'
        elif 2.5 <= mean['HTR'] <= 3.4:
            mean['HTR'] = 'A'
    input_df = pd.DataFrame([mean])
    return align_features(input_df, model)

def predict_with_confidence(model, input_df):
    try:
        proba = model.predict_proba(input_df)[0]
        pred_idx = proba.argmax()
        labels = model.classes_
        return labels[pred_idx], proba[pred_idx], dict(zip(labels, proba))
    except Exception as e:
        logging.error(f"Prediction error: {e}")
        return None, None, None

def determine_final_prediction(pred, probs, home_team, away_team):
    # Step 1: Get model prediction
    if 0.5 <= pred <= 1.4:
        model_outcome = f"{home_team} win"
    elif 1.5 <= pred <= 2.4:
        model_outcome = "Draw"
    elif 2.5 <= pred <= 3.4:
        model_outcome = f"{away_team} win"
    else:
        return "â— Invalid prediction"

    # Step 2: Find highest probability outcomes
    max_prob = max(probs.values())
    highest_outcomes = [k for k, v in probs.items() if v == max_prob]

    # Convert generic labels to team-specific labels
    outcome_mapping = {
        "Home Team Win": f"{home_team} win",
        "Away Team Win": f"{away_team} win",
        "Draw": "Draw"
    }
    
    # Convert to team-specific outcomes
    highest_outcomes = [outcome_mapping.get(outcome, outcome) for outcome in highest_outcomes]
    model_outcome = outcome_mapping.get(model_outcome, model_outcome)

    # Case 1: Clear winner (no tie)
    if len(highest_outcomes) == 1:
        return highest_outcomes[0]

    # Case 2: Home & Away are tied
    if f"{home_team} win" in highest_outcomes and f"{away_team} win" in highest_outcomes:
        return f"{home_team} or {away_team} win"

    # Case 3: Home & Draw are tied
    if f"{home_team} win" in highest_outcomes and "Draw" in highest_outcomes:
        if model_outcome == f"{away_team} win":
            return f"{home_team} win or Draw"
        return model_outcome if model_outcome in highest_outcomes else f"{home_team} win or Draw"

    # Case 4: Away & Draw are tied
    if f"{away_team} win" in highest_outcomes and "Draw" in highest_outcomes:
        if model_outcome == f"{home_team} win":
            return f"{away_team} win or Draw"
        return model_outcome if model_outcome in highest_outcomes else f"{away_team} win or Draw"

    # Case 5: All three outcomes tied (trust model)
    if len(highest_outcomes) == 3:
        return model_outcome

    # Fallback
    return f"{model_outcome} (Uncertain)"